{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733aa4f7",
      "metadata": {
        "id": "733aa4f7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Conv1D, MaxPooling1D, Flatten, Dense, Input, Dropout, LSTM, Bidirectional, Activation, RepeatVector, Permute, Multiply, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9xqzXykI8hq4",
      "metadata": {
        "id": "9xqzXykI8hq4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6c04e3",
      "metadata": {
        "id": "5b6c04e3"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "`FILE_PATH`: file containing the data. \\\n",
        "`FS`: the hertz used in the file. \\\n",
        "`GROUP`: The ID of the group to run. \\\n",
        "`OVERLAP`: overlap value used for segmentation. Between 0 and 1. \\\n",
        "`PROCESSES`: number of processes to use for segmentation. \\\n",
        "`FLAG_SPLIT`: 0 - intra-subject, 1 - inter-subject, 2-inter-session. \\\n",
        "`LR`: Learning rate to use for models. \\\n",
        "`BATCH_SIZE`: Batch size to use for models. \\\n",
        "`EPOCHS`: Epochs to use for models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baefca68",
      "metadata": {
        "id": "baefca68"
      },
      "outputs": [],
      "source": [
        "FILE_PATH = '/content/drive/My Drive/MASTER/RAWFILES/UiS4ADL-100hz.csv'\n",
        "FS = 100\n",
        "\n",
        "GROUP = 1\n",
        "OVERLAP = 0.5\n",
        "FLAG_SPLIT = 0\n",
        "\n",
        "PROCESSES = 10\n",
        "LR = 0.00001\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad73d195",
      "metadata": {
        "id": "ad73d195"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e46b9e0",
      "metadata": {
        "id": "3e46b9e0"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(FILE_PATH)\n",
        "data = data.drop(columns='timestamp', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2460ee2",
      "metadata": {
        "id": "f2460ee2"
      },
      "source": [
        "### Data to drop\n",
        "Dropping subjects data because of incorrect data recording. \\\n",
        "**Change variable `subjects_to_drop` accordingly to goal.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2889967",
      "metadata": {
        "id": "b2889967"
      },
      "outputs": [],
      "source": [
        "subjects_to_drop = [1727,1826,2097]\n",
        "to_drop = data[data['subject'].isin(subjects_to_drop)]\n",
        "data.drop(to_drop.index, inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0be9f6",
      "metadata": {
        "id": "2b0be9f6"
      },
      "source": [
        "### Analysis if there's missing data, and drop it if there's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa204b17",
      "metadata": {
        "id": "fa204b17"
      },
      "outputs": [],
      "source": [
        "to_drop = data[data.isna().any(axis=1)]\n",
        "print(to_drop.subject.unique(),to_drop.session.unique(),to_drop.adl.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f51a1cc3",
      "metadata": {
        "id": "f51a1cc3"
      },
      "outputs": [],
      "source": [
        "data.drop(to_drop.index,inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8iclK5Dyvjcw",
      "metadata": {
        "id": "8iclK5Dyvjcw"
      },
      "source": [
        "## Choose group\n",
        "\n",
        "**Change GROUP variables accordingly to analysis.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xC_pQYAJvmU4",
      "metadata": {
        "id": "xC_pQYAJvmU4"
      },
      "outputs": [],
      "source": [
        "if GROUP == 0:\n",
        "    adls_in_group = [1, 3, 4, 7, 10, 11, 12, 13, 14, 16, 19, 20]\n",
        "    features_in_group = ['accX[mg]', 'magnX[mG]', 'fxPitch[deg]', 'magnZ[mG]', 'fxQ3', 'magnY[mG]', 'fxQ4', 'fxQ2', 'fxQ1', 'fxGravityZ[g]']\n",
        "    w = 2 # window size (number of samples)\n",
        "if GROUP == 1:\n",
        "    adls_in_group = [6, 8, 9, 18, 21, 24]\n",
        "    features_in_group = ['magnX[mG]', 'fxPitch[deg]', 'accX[mg]', 'magnY[mG]', 'magnZ[mG]', 'fxQ4', 'fxQ3', 'fxQ1']\n",
        "    w = 5 # window size (number of samples)\n",
        "if GROUP == 2:\n",
        "    adls_in_group = [2, 5, 15, 17, 22, 23]\n",
        "    features_in_group = ['fxPitch[deg]', 'accX[mg]', 'magnX[mG]', 'magnY[mG]', 'magnZ[mG]', 'fxGravityZ[g]']\n",
        "    w = 10 # window size (number of samples)\n",
        "\n",
        "data = data[data['adl'].isin(adls_in_group)]\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e955e1ef",
      "metadata": {
        "id": "e955e1ef"
      },
      "source": [
        "## Downsample to 32 Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0866f7",
      "metadata": {
        "id": "ca0866f7"
      },
      "outputs": [],
      "source": [
        "FS = 32\n",
        "# Define a function for downsampling\n",
        "def downsample_group(group):\n",
        "    return group.iloc[::3]\n",
        "\n",
        "# Apply the downsampling function separately for each 'fileID'\n",
        "data = data.groupby('fileID').apply(downsample_group)\n",
        "\n",
        "# Reset index if needed\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4829bdf",
      "metadata": {
        "id": "f4829bdf"
      },
      "source": [
        "## Feature selection\n",
        "\n",
        "**Change GROUP variables accordingly to analysis.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c8f69e",
      "metadata": {
        "id": "75c8f69e"
      },
      "outputs": [],
      "source": [
        "columns = features_in_group\n",
        "columns.extend(['adl', 'session', 'subject', 'fileID'])\n",
        "data = data[columns]\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2319d249",
      "metadata": {
        "id": "2319d249"
      },
      "source": [
        "## Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747110fc",
      "metadata": {
        "id": "747110fc"
      },
      "source": [
        "**Change variable `directory` and `filename` accordingly to filepath of segment / where to save it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XxJj9TgUdqOB",
      "metadata": {
        "id": "XxJj9TgUdqOB"
      },
      "outputs": [],
      "source": [
        "directory = \"/content/drive/My Drive/MASTER/SEGMENT/3/\"\n",
        "filename = \"UiS4ADL_seg_w_\" + str(w) + \"ov_\" + str(int(OVERLAP*100)) + \".csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf65a977",
      "metadata": {
        "id": "bf65a977"
      },
      "outputs": [],
      "source": [
        "def segment_data(data, w, fs, overlap):\n",
        "    window_size = w * fs\n",
        "    overlap_size = int(window_size * overlap)\n",
        "    step_size = window_size - overlap_size\n",
        "\n",
        "    segments = []\n",
        "    i = 0\n",
        "    while i + window_size <= len(data):\n",
        "        segment = data.iloc[i:i+window_size, :]\n",
        "        segments.append(pd.DataFrame(np.concatenate([segment.iloc[[i]].values for i in range(len(segment))], axis=1)))\n",
        "        i += step_size\n",
        "\n",
        "    return pd.concat(segments,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2450366",
      "metadata": {
        "id": "e2450366"
      },
      "outputs": [],
      "source": [
        "# Define a function to process each fileID\n",
        "def process_file(fileID, data, w, fs):\n",
        "    tmp_data = data[data.fileID == fileID]\n",
        "    tmp2 = tmp_data.iloc[0:,-4:]\n",
        "\n",
        "    tmp = tmp_data.iloc[:, 0:-4]\n",
        "\n",
        "    segmented_data = []\n",
        "    if len(tmp) >= w * fs:\n",
        "        tmp = segment_data(tmp, w, fs, OVERLAP)\n",
        "        tmp['adl'] = tmp2.iloc[0,0]\n",
        "        tmp['session'] = tmp2.iloc[0,1]\n",
        "        tmp['subject'] = tmp2.iloc[0,2]\n",
        "        tmp['fileID'] = fileID\n",
        "        segmented_data.append(tmp)\n",
        "\n",
        "    return segmented_data\n",
        "\n",
        "file_path = os.path.join(directory, filename)\n",
        "if not os.path.exists(file_path):\n",
        "  # Number of processes to use\n",
        "  num_processes = PROCESSES\n",
        "\n",
        "  # Get unique values of the 'Series' column\n",
        "  unique_series = data.fileID.unique()\n",
        "\n",
        "  # Initialize progress bar\n",
        "  pbar = tqdm(total=len(unique_series), desc='Progress')\n",
        "\n",
        "  # Create a partial function with fixed w and fs arguments\n",
        "  partial_process_file = partial(process_file, data=data, w=w, fs=FS)\n",
        "\n",
        "  # Create a pool of processes and map the function to unique_series\n",
        "  with Pool(num_processes) as pool:\n",
        "      results = pool.map(partial_process_file, unique_series)\n",
        "\n",
        "  # Concatenate results\n",
        "  segmented_data = [item for sublist in results for item in sublist]\n",
        "  data = pd.concat(segmented_data, axis=0).reset_index(drop=True)\n",
        "\n",
        "  # Close progress bar\n",
        "  pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d845021",
      "metadata": {
        "id": "0d845021"
      },
      "outputs": [],
      "source": [
        "def save_dataframe_to_csv(directory, filename, df):\n",
        "    # Check if the directory exists, if not, create it\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Check if the file exists in the directory\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        # Save the DataFrame as CSV\n",
        "        df.to_csv(file_path, index=False)\n",
        "        print(f\"DataFrame saved as {filename} in {directory}\")\n",
        "    else:\n",
        "        print(f\"File {filename} already exists in {directory}\")\n",
        "\n",
        "file_path = os.path.join(directory, filename)\n",
        "if os.path.exists(file_path):\n",
        "    data = pd.read_csv(directory + filename)\n",
        "else:\n",
        "    save_dataframe_to_csv(directory, filename, data)\n",
        "    data = pd.read_csv(directory + filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3158599a",
      "metadata": {
        "id": "3158599a"
      },
      "source": [
        "## Overview of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2f113e",
      "metadata": {
        "id": "ac2f113e"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset Overview:\")\n",
        "display((data.head()))\n",
        "print(data.shape)\n",
        "print()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Values:\")\n",
        "print(data.isnull().sum().sum())\n",
        "print()\n",
        "\n",
        "# Basic statistics of the numerical columns\n",
        "print(\"Basic Statistics:\")\n",
        "display(data.describe().iloc[0:,1:-4])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8635af",
      "metadata": {
        "id": "5e8635af"
      },
      "source": [
        "## Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60eaf3db",
      "metadata": {
        "id": "60eaf3db"
      },
      "outputs": [],
      "source": [
        "def display_distribution(data,column,fs,w):\n",
        "    plt.figure(figsize=(16, 5))  # Adjust the figure size as per your preference\n",
        "    counts = data[column].value_counts().sort_index()\n",
        "    counts.plot(kind='bar')\n",
        "\n",
        "    for i, v in enumerate(counts):\n",
        "        plt.text(i, v, str(v) + '(' + str(int(round((v/len(data))*100,0)))+'%' + ')', ha='center', va='bottom')\n",
        "\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Distribution of ' + column + ' samples in terms of segments of '+ str(w) + ' second')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e51eef",
      "metadata": {
        "id": "71e51eef"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'adl',FS,w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc89af4",
      "metadata": {
        "id": "efc89af4"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'subject',FS, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8dee02d",
      "metadata": {
        "id": "d8dee02d"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'session',FS, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c576b5de",
      "metadata": {
        "id": "c576b5de"
      },
      "source": [
        "## Undersample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94b8346",
      "metadata": {
        "id": "d94b8346"
      },
      "outputs": [],
      "source": [
        "adl_distribution = data['adl'].value_counts().sort_index()\n",
        "adl_smallest_value = adl_distribution.min()\n",
        "\n",
        "rus = RandomUnderSampler(sampling_strategy='not minority', random_state=42)\n",
        "X = data.drop(columns=['adl'])\n",
        "y = data['adl']\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "\n",
        "X_resampled['adl'] = y_resampled\n",
        "data = X_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d37439",
      "metadata": {
        "id": "29d37439"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'adl',FS,w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0bc9d0",
      "metadata": {
        "id": "5b0bc9d0"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'subject',FS,w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701a1507",
      "metadata": {
        "id": "701a1507"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'session',FS, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b09e3f",
      "metadata": {
        "id": "e7b09e3f"
      },
      "source": [
        "### Rename activities from 0 to N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a531c31b",
      "metadata": {
        "id": "a531c31b"
      },
      "outputs": [],
      "source": [
        "unique_values = data['adl'].unique()\n",
        "codes, unique_labels = pd.factorize(unique_values)\n",
        "value_mapping = dict(zip(unique_values, codes))\n",
        "data['adl'] = data['adl'].map(value_mapping)\n",
        "old_new_mapping = dict(zip(unique_values, unique_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e715ee69",
      "metadata": {
        "id": "e715ee69"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix( y_test, y_pred):\n",
        "    classification_report_df = pd.DataFrame(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1),output_dict=True)).transpose()\n",
        "    new_mapping = sorted(value_mapping.keys())\n",
        "    new_mapping.extend(['accuracy', 'macro avg', 'weighted avg'])\n",
        "    classification_report_df.index = new_mapping\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    display(classification_report_df)\n",
        "\n",
        "    cm = np.round(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1),normalize='true')*100,2)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g',\n",
        "                xticklabels=new_mapping[:-3], yticklabels=new_mapping[:-3])\n",
        "\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_validation_history(history):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(8, 2))\n",
        "\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "    axs[0].set_xlabel(\"Epoch\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Loss\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def time_model_inference(model: Model, input_data):\n",
        "    start_time = time.time()\n",
        "    model.predict(input_data)\n",
        "    end_time = time.time()\n",
        "    print('Elapsed Time ', end_time - start_time)\n",
        "    print('# of Test samples', len(input_data))\n",
        "    print('Average Inference time (ms):', (end_time - start_time)/len(input_data)*1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02db8727",
      "metadata": {
        "id": "02db8727"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_the_model(data):\n",
        "    X = data.iloc[0:,0:-4]\n",
        "    y = data['adl']\n",
        "\n",
        "    subject = data['subject'].values\n",
        "    session = data['session'].values\n",
        "    adl = data['adl'].values\n",
        "    adl_encoder = LabelEncoder()\n",
        "    adl = adl_encoder.fit_transform(adl)\n",
        "\n",
        "    if FLAG_SPLIT == 2:\n",
        "        train_session, test_session = train_test_split(np.unique(session), test_size=0.2, random_state=28)\n",
        "        train_session, val_session = train_test_split(train_session, test_size=0.2, random_state=28)\n",
        "\n",
        "        train_idx = np.where(np.isin(session, train_session))[0]\n",
        "        val_idx = np.where(np.isin(session, val_session))[0]\n",
        "        test_idx = np.where(np.isin(session, test_session))[0]\n",
        "\n",
        "        print('Train Sessions: ', train_session)\n",
        "        print('Validation Sessions: ', val_session)\n",
        "        print('Test Sessions: ', test_session)\n",
        "\n",
        "        X_train, X_val, X_test = X.loc[train_idx], X.loc[val_idx], X.loc[test_idx]\n",
        "        y_train, y_val, y_test = y.loc[train_idx], y.loc[val_idx], y.loc[test_idx]\n",
        "\n",
        "    elif FLAG_SPLIT == 1:\n",
        "        train_subjects, test_subjects = train_test_split(np.unique(subject), test_size=0.2, random_state=28)\n",
        "        train_subjects, val_subjects = train_test_split(train_subjects, test_size=0.25, random_state=28)\n",
        "\n",
        "        train_idx = np.where(np.isin(subject, train_subjects))[0]\n",
        "        val_idx = np.where(np.isin(subject, val_subjects))[0]\n",
        "        test_idx = np.where(np.isin(subject, test_subjects))[0]\n",
        "\n",
        "        print(\"Train Subjects :\", train_subjects)\n",
        "        print(\"Validation Subjects :\", val_subjects)\n",
        "        print(\"Test Subjects :\", test_subjects)\n",
        "\n",
        "        X_train, X_val, X_test = X.loc[train_idx], X.loc[val_idx], X.loc[test_idx]\n",
        "        y_train, y_val, y_test = y.loc[train_idx], y.loc[val_idx], y.loc[test_idx]\n",
        "\n",
        "    elif FLAG_SPLIT == 0:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
        "\n",
        "    X_train = X_train.values\n",
        "    X_val = X_val.values\n",
        "    X_test = X_test.values\n",
        "\n",
        "    num_classes = len(adl_encoder.classes_)\n",
        "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "    y_val = to_categorical(y_val, num_classes=num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "\n",
        "    input_shape = (X_train[0].shape)\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], *input_shape)\n",
        "    X_val = X_val.reshape(X_val.shape[0], *input_shape)\n",
        "    X_test = X_test.reshape(X_test.shape[0], *input_shape)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, num_classes, input_shape\n",
        "\n",
        "\n",
        "def evaluate_my_model(model, history, X_test, y_test):\n",
        "    plot_training_validation_history(history)\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "    y_pred =  np.round(model.predict(X_test, batch_size=None, verbose=\"auto\", steps=None, callbacks=None))\n",
        "    plot_confusion_matrix(y_test, y_pred)\n",
        "    time_model_inference(model,X_test)\n",
        "\n",
        "def my_LSTM_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=(X_train[0].shape)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_CNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train[0].shape), padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(256, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_CNNRNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=256, kernel_size=3, activation=\"relu\", input_shape=(X_train[0].shape)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(256)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_RNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(128, input_shape=(X_train[0].shape), return_sequences=True))\n",
        "    model.add(SimpleRNN(128, return_sequences=True))\n",
        "    model.add(SimpleRNN(128, return_sequences=True))\n",
        "    model.add(SimpleRNN(128))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_MLP_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    flatten_layer = Flatten()(input_layer)\n",
        "    hidden_layer1 = Dense(32, activation='relu')(flatten_layer)\n",
        "    dropout1 = Dropout(0.4)(hidden_layer1)\n",
        "    hidden_layer2 = Dense(64, activation='relu')(dropout1)\n",
        "    dropout2 = Dropout(0.4)(hidden_layer2)\n",
        "    hidden_layer3 = Dense(128, activation='relu')(dropout2)\n",
        "    dropout3 = Dropout(0.4)(hidden_layer3)\n",
        "    hidden_layer4 = Dense(256, activation='relu')(dropout3)\n",
        "    dropout4 = Dropout(0.4)(hidden_layer4)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dropout4)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_LSTMBiAtt_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    input_layer = Input(shape=(X_train[0].shape))\n",
        "    lstm_layer = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
        "    attention = Dense(1, activation='tanh')(lstm_layer)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(256)(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "    attention_mul = Multiply()([lstm_layer, attention])\n",
        "    output_layer = LSTM(128)(attention_mul)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(output_layer)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa67f9d",
      "metadata": {
        "id": "1aa67f9d"
      },
      "source": [
        "## Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0bcc22",
      "metadata": {
        "id": "4d0bcc22"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, num_classes, input_shape = prepare_data_for_the_model(data)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\",  X_test.shape)\n",
        "\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"y_test shape:\",  y_test.shape)\n",
        "\n",
        "print(\"Number of classes:\", num_classes)\n",
        "print(\"Input shape:\", input_shape)\n",
        "\n",
        "X_train = X_train.reshape(len(X_train),-1,2)\n",
        "X_val = X_val.reshape(len(X_val),-1,2)\n",
        "X_test = X_test.reshape(len(X_test),-1,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ef0b64",
      "metadata": {
        "id": "19ef0b64"
      },
      "source": [
        "## Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4879e3d8",
      "metadata": {
        "id": "4879e3d8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "my_CNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e271c9e1",
      "metadata": {
        "id": "e271c9e1"
      },
      "outputs": [],
      "source": [
        "my_CNNRNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c692099",
      "metadata": {
        "id": "0c692099"
      },
      "outputs": [],
      "source": [
        "my_LSTM_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc3b945",
      "metadata": {
        "id": "2fc3b945"
      },
      "outputs": [],
      "source": [
        "my_RNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cddc02f",
      "metadata": {
        "id": "1cddc02f"
      },
      "outputs": [],
      "source": [
        "my_MLP_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9809a2c4",
      "metadata": {
        "id": "9809a2c4"
      },
      "outputs": [],
      "source": [
        "my_LSTMBiAtt_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1sJzfFntfGC6"
      },
      "id": "1sJzfFntfGC6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4450820,
          "sourceId": 7637398,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}