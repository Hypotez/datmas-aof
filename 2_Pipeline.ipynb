{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yr5WuKGAUHNs",
      "metadata": {
        "id": "yr5WuKGAUHNs"
      },
      "outputs": [],
      "source": [
        "!pip install tsfel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733aa4f7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:12:08.001512Z",
          "iopub.status.busy": "2024-03-06T11:12:08.001111Z",
          "iopub.status.idle": "2024-03-06T11:12:10.611853Z",
          "shell.execute_reply": "2024-03-06T11:12:10.610638Z",
          "shell.execute_reply.started": "2024-03-06T11:12:08.001482Z"
        },
        "id": "733aa4f7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import tsfel\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Conv1D, MaxPooling1D, Flatten, Dense, Input, Dropout, LSTM, Bidirectional, Activation, RepeatVector, Permute, Multiply, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9xqzXykI8hq4",
      "metadata": {
        "id": "9xqzXykI8hq4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6c04e3",
      "metadata": {
        "id": "5b6c04e3"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "`FILE_PATH`: file containing the data. \\\n",
        "`FS`: the hertz used in the file. \\\n",
        "`GROUP`: The ID of the group to run. \\\n",
        "`OVERLAP`: overlap value used for segmentation. Between 0 and 1. \\\n",
        "`FLAG_SPLIT`: 0 - intra-subject, 1 - inter-subject, 2-inter-session. \\\n",
        "`LR`: Learning rate to use for models. \\\n",
        "`BATCH_SIZE`: Batch size to use for models. \\\n",
        "`EPOCHS`: Epochs to use for models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baefca68",
      "metadata": {
        "id": "baefca68"
      },
      "outputs": [],
      "source": [
        "FILE_PATH = '/content/drive/UiS4ADL-100hz.csv'\n",
        "FS = 100\n",
        "\n",
        "GROUP = 0\n",
        "OVERLAP = 0.5\n",
        "FLAG_SPLIT = 2\n",
        "\n",
        "LR = 0.00001\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad73d195",
      "metadata": {
        "id": "ad73d195"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e46b9e0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:12:10.614664Z",
          "iopub.status.busy": "2024-03-06T11:12:10.613959Z",
          "iopub.status.idle": "2024-03-06T11:12:37.702873Z",
          "shell.execute_reply": "2024-03-06T11:12:37.701620Z",
          "shell.execute_reply.started": "2024-03-06T11:12:10.614619Z"
        },
        "id": "3e46b9e0"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(FILE_PATH)\n",
        "data = data.drop(columns='timestamp', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2460ee2",
      "metadata": {
        "id": "f2460ee2"
      },
      "source": [
        "### Data to drop\n",
        "Dropping subjects data because of incorrect data recording. \\\n",
        "**Change variable `subjects_to_drop` accordingly to goal.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2889967",
      "metadata": {
        "id": "b2889967"
      },
      "outputs": [],
      "source": [
        "subjects_to_drop = [1727,1826,2097]\n",
        "to_drop = data[data['subject'].isin(subjects_to_drop)]\n",
        "data.drop(to_drop.index, inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0be9f6",
      "metadata": {
        "id": "2b0be9f6"
      },
      "source": [
        "### Analysis if there's missing data, and drop it if there's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa204b17",
      "metadata": {
        "id": "fa204b17"
      },
      "outputs": [],
      "source": [
        "to_drop = data[data.isna().any(axis=1)]\n",
        "print(to_drop.subject.unique(),to_drop.session.unique(),to_drop.adl.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f51a1cc3",
      "metadata": {
        "id": "f51a1cc3"
      },
      "outputs": [],
      "source": [
        "data.drop(to_drop.index,inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8iclK5Dyvjcw",
      "metadata": {
        "id": "8iclK5Dyvjcw"
      },
      "source": [
        "## Choose group\n",
        "\n",
        "**Change GROUP variables accordingly to analysis.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xC_pQYAJvmU4",
      "metadata": {
        "id": "xC_pQYAJvmU4"
      },
      "outputs": [],
      "source": [
        "if GROUP == 0:\n",
        "    adls_in_group = [1, 3, 4, 7, 10, 11, 12, 13, 14, 16, 19, 20]\n",
        "    w = 2 # window size (number of samples)\n",
        "if GROUP == 1:\n",
        "    adls_in_group = [6, 8, 9, 18, 21, 24]\n",
        "    w = 5 # window size (number of samples)\n",
        "if GROUP == 2:\n",
        "    adls_in_group = [2, 5, 15, 17, 22, 23]\n",
        "    w = 10 # window size (number of samples)\n",
        "\n",
        "data = data[data['adl'].isin(adls_in_group)]\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e955e1ef",
      "metadata": {
        "id": "e955e1ef"
      },
      "source": [
        "## Downsample to 32 Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0866f7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:12:48.011216Z",
          "iopub.status.busy": "2024-03-06T11:12:48.010846Z",
          "iopub.status.idle": "2024-03-06T11:12:49.473681Z",
          "shell.execute_reply": "2024-03-06T11:12:49.472476Z",
          "shell.execute_reply.started": "2024-03-06T11:12:48.011186Z"
        },
        "id": "ca0866f7"
      },
      "outputs": [],
      "source": [
        "FS = 32\n",
        "# Define a function for downsampling\n",
        "def downsample_group(group):\n",
        "    return group.iloc[::3]\n",
        "\n",
        "# Apply the downsampling function separately for each 'fileID'\n",
        "data = data.groupby('fileID').apply(downsample_group)\n",
        "\n",
        "# Reset index if needed\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4829bdf",
      "metadata": {
        "id": "f4829bdf"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gxvvXf3Y08hq",
      "metadata": {
        "id": "gxvvXf3Y08hq"
      },
      "source": [
        "**Change variable `directory` and `filename` accordingly to filepath of segment / where to save it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cVKsiqE0BhS8",
      "metadata": {
        "id": "cVKsiqE0BhS8"
      },
      "outputs": [],
      "source": [
        "directory = \"/content/drive/My Drive/MASTER/SEGMENT_2/\"\n",
        "filename = \"UiS4ADL_seg_w_\" + str(w) + \"ov_\" + str(int(OVERLAP*100)) + \".csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c8f69e",
      "metadata": {
        "id": "75c8f69e"
      },
      "outputs": [],
      "source": [
        "def save_dataframe_to_csv(directory, filename, df):\n",
        "\n",
        "    # Check if the file exists in the directory\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    # Save the DataFrame as CSV\n",
        "    df.to_csv(file_path, mode='a', index=False, header= not os.path.exists(file_path))\n",
        "    print(f\"DataFrame saved as {filename} in {directory}\")\n",
        "\n",
        "file_path = os.path.join(directory, filename)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "  #Use statistical features\n",
        "  cfg_file = tsfel.get_features_by_domain('statistical')\n",
        "\n",
        "  for window in [w]:\n",
        "    for o in [OVERLAP]:\n",
        "      windows_size = int(window * FS)\n",
        "      for fileID in data.fileID.unique():\n",
        "        #Get unique files.\n",
        "        tmp = data[data.fileID==fileID].iloc[0:,0:-4]\n",
        "\n",
        "        #The window size is sufficient.\n",
        "        if tmp.shape[0]>=windows_size:\n",
        "\n",
        "          #Extract features.\n",
        "          tmp_features = tsfel.time_series_features_extractor(cfg_file, tmp, fs = FS, window_size=windows_size, overlap=o, header_names = tmp.columns.values, verbose=False, n_jobs = -1)\n",
        "\n",
        "          #Add ADL, Session, Subject and fileID to dataframe.\n",
        "          tmp_features['adl'] = data[data.fileID==fileID].iloc[0,-4]\n",
        "          tmp_features['session'] = data[data.fileID==fileID].iloc[0,-3]\n",
        "          tmp_features['subject'] = data[data.fileID==fileID].iloc[0,-2]\n",
        "          tmp_features['fileID'] = fileID\n",
        "\n",
        "          #Save file to csv\n",
        "          print('------------------------------------------------------------------------------------------------------')\n",
        "          print('Extracted Features from Series ' + str(fileID) + ' and window ' + str(w))\n",
        "          print('------------------------------------------------------------------------------------------------------')\n",
        "          save_dataframe_to_csv(directory, filename, tmp_features)\n",
        "          print('------------------------------------------------------------------------------------------------------')\n",
        "        else:\n",
        "          print('FileID --> ' + str(fileID) + ' it to short. Only '+ str(tmp.shape[0]) + ' rows (aka. ' + str(tmp.shape[0]/FS) +' seconds).')\n",
        "  data = pd.read_csv(directory + filename)\n",
        "else:\n",
        "  data = pd.read_csv(directory + filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5wLGTTCvNmVP",
      "metadata": {
        "id": "5wLGTTCvNmVP"
      },
      "outputs": [],
      "source": [
        "display(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X-luSvoBPaaE",
      "metadata": {
        "id": "X-luSvoBPaaE"
      },
      "source": [
        "## Overview of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xwdhAQ-gRCDh",
      "metadata": {
        "id": "xwdhAQ-gRCDh"
      },
      "outputs": [],
      "source": [
        "to_drop = data[data.isna().any(axis=1)]\n",
        "print(to_drop.subject.unique(),to_drop.session.unique(),to_drop.adl.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Dgwk0hyRCpS",
      "metadata": {
        "id": "8Dgwk0hyRCpS"
      },
      "outputs": [],
      "source": [
        "data.drop(to_drop.index,inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IR5zQDNeP-XY",
      "metadata": {
        "id": "IR5zQDNeP-XY"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset Overview:\")\n",
        "display((data.head()))\n",
        "print(data.shape)\n",
        "print()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Values:\")\n",
        "print(data.isnull().sum().sum())\n",
        "print()\n",
        "\n",
        "# Basic statistics of the numerical columns\n",
        "print(\"Basic Statistics:\")\n",
        "display(data.describe().iloc[0:,0:-4])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrnBiKLkRazC",
      "metadata": {
        "id": "wrnBiKLkRazC"
      },
      "source": [
        "## Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lCVegYuYRoDc",
      "metadata": {
        "id": "lCVegYuYRoDc"
      },
      "source": [
        "### Feature variance for all ADLs in group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HALH_bO0RdUq",
      "metadata": {
        "id": "HALH_bO0RdUq"
      },
      "outputs": [],
      "source": [
        "variance_features_df = {\n",
        "   \"Feature\": data.columns.to_list()[0:-4]\n",
        "    }\n",
        "variance_features_df = pd.DataFrame(variance_features_df)\n",
        "\n",
        "variance_features = data.iloc[0:,0:-4].var()\n",
        "variance_features_df['Variance'] = variance_features.values\n",
        "sorted_variances = variance_features_df.sort_values('Variance')\n",
        "\n",
        "display(sorted_variances)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sIPmW7qraF6V",
      "metadata": {
        "id": "sIPmW7qraF6V"
      },
      "source": [
        "### Feature variance based on ADL ID\n",
        "\n",
        "List the feature to remove for each ADL ID based on variance. \\\n",
        "1 is remove, 0 is to keep \\\n",
        "**Change variable `variance_threshold` accordingly to goal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3TCVJ19oZnvq",
      "metadata": {
        "id": "3TCVJ19oZnvq"
      },
      "outputs": [],
      "source": [
        "feature_remove = {\n",
        "   \"Feature_remove\": data.columns.to_list()[0:-4]\n",
        "    }\n",
        "feature_remove = pd.DataFrame(feature_remove)\n",
        "\n",
        "variance_threshold = 0.00\n",
        "for adl in data.adl.unique():\n",
        "    variance_features = data[data['adl'] == adl].iloc[0:,0:-4].var()\n",
        "    feature_keep_index = variance_features[variance_features > variance_threshold].index.tolist()\n",
        "\n",
        "    result = []\n",
        "    for feature in feature_remove['Feature_remove']:\n",
        "        if feature in feature_keep_index:\n",
        "            result.append(0)\n",
        "        else:\n",
        "            result.append(1)\n",
        "    feature_remove[adl] = result\n",
        "\n",
        "feature_remove['Total'] = feature_remove.iloc[0:,1:].sum(axis=1)\n",
        "feature_column = feature_remove.sum(axis=0).values\n",
        "feature_column[0] = 'Total'\n",
        "feature_remove.loc[len(feature_remove)] = feature_column\n",
        "display(feature_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fp3RZ6bY3n2",
      "metadata": {
        "id": "9fp3RZ6bY3n2"
      },
      "source": [
        "Remove the features which have been recognized to have low variance in one of the ADLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ov4I91zQY3Pb",
      "metadata": {
        "id": "Ov4I91zQY3Pb"
      },
      "outputs": [],
      "source": [
        "drop_features = []\n",
        "\n",
        "new_feature_remove = feature_remove.iloc[:-1, :]\n",
        "for i in range(len(new_feature_remove['Total'])):\n",
        "  if new_feature_remove['Total'][i] > 0:\n",
        "    drop_features.append(new_feature_remove['Feature_remove'][i])\n",
        "\n",
        "print(len(drop_features))\n",
        "feature_remove = new_feature_remove[~new_feature_remove['Feature_remove'].isin(drop_features)]\n",
        "feature_remove.reset_index(drop=True, inplace=True)\n",
        "\n",
        "feature_column = feature_remove.sum(axis=0).values\n",
        "feature_column[0] = 'Total'\n",
        "feature_remove.loc[len(feature_remove)] = feature_column\n",
        "\n",
        "data = data.drop(columns=drop_features, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uIdricGkhhoQ",
      "metadata": {
        "id": "uIdricGkhhoQ"
      },
      "source": [
        "### Feature correlation based on ADL ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0NR5qUtnhlfU",
      "metadata": {
        "id": "0NR5qUtnhlfU"
      },
      "source": [
        "The features with highest variance will be checked first for correlation.\n",
        "\n",
        "**Change variable `correlation_threshold` accordingly to goal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k7g3HpjGhjFq",
      "metadata": {
        "id": "k7g3HpjGhjFq"
      },
      "outputs": [],
      "source": [
        "correlation_threshold = 0.95\n",
        "\n",
        "new_feature_remove = feature_remove.iloc[:-1,:-1]\n",
        "for adl in data.adl.unique():\n",
        "    feature_columns = data.columns.tolist()[0:-4]\n",
        "\n",
        "    #Finding variance and sort by descending\n",
        "    variance_features_df = {\n",
        "    \"Feature\": feature_columns\n",
        "    }\n",
        "    variance_features_df = pd.DataFrame(variance_features_df)\n",
        "\n",
        "    #Adding adl such that we can select based on adl.\n",
        "    feature_columns.extend([\"adl\"])\n",
        "    data_copy = data[feature_columns]\n",
        "\n",
        "    variance_features = data_copy[data_copy['adl'] == adl].var()\n",
        "    variance_features = variance_features.drop(\"adl\")\n",
        "    variance_features_df['Variance'] = variance_features.values\n",
        "\n",
        "    variance_features_df = variance_features_df.sort_values('Variance', ascending=False)\n",
        "    highest_variance_features = variance_features_df['Feature'].tolist()\n",
        "\n",
        "    #Adding adl such that we can select based on adl.\n",
        "    highest_variance_features.extend([\"adl\"])\n",
        "\n",
        "    #Restructure data copy to have same order as highest variance features.\n",
        "    data_copy = data_copy.reindex(columns=highest_variance_features)\n",
        "\n",
        "    correlation_matrix = data_copy[data_copy['adl'] == adl].corr()\n",
        "    correlation_matrix = correlation_matrix.iloc[:-1,:-1]\n",
        "\n",
        "    #Remove the high correlation features and only keep one left.\n",
        "    #Keep the one with highest variance.\n",
        "\n",
        "    correlation_features_remove = []\n",
        "    for column in correlation_matrix.columns:\n",
        "        if column in correlation_features_remove:\n",
        "            continue\n",
        "\n",
        "        feature_column = correlation_matrix[column].abs()\n",
        "        selected_features = feature_column[feature_column > correlation_threshold].index.tolist()\n",
        "        filtered_features = [item for item in selected_features if item != column]\n",
        "\n",
        "        for feature in filtered_features:\n",
        "            if feature not in correlation_features_remove:\n",
        "                correlation_features_remove.append(feature)\n",
        "\n",
        "    for feature in correlation_features_remove:\n",
        "        for j in range(len(new_feature_remove['Feature_remove'].values)):\n",
        "            if feature == new_feature_remove['Feature_remove'].values[j]:\n",
        "                new_feature_remove[adl][j] = 1\n",
        "                break\n",
        "\n",
        "new_feature_remove['Total'] = new_feature_remove.iloc[0:,1:].sum(axis=1)\n",
        "feature_column = new_feature_remove.sum(axis=0).values\n",
        "feature_column[0] = 'Total'\n",
        "new_feature_remove.loc[len(new_feature_remove)] = feature_column\n",
        "feature_remove = new_feature_remove\n",
        "display(feature_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JOdXYsrte9Iy",
      "metadata": {
        "id": "JOdXYsrte9Iy"
      },
      "source": [
        "Removing high correlation features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SngCqB2wfUeg",
      "metadata": {
        "id": "SngCqB2wfUeg"
      },
      "outputs": [],
      "source": [
        "drop_features = []\n",
        "\n",
        "new_feature_remove = feature_remove.iloc[:-1, :]\n",
        "for i in range(len(new_feature_remove['Total'])):\n",
        "  if new_feature_remove['Total'][i] > 0:\n",
        "    drop_features.append(new_feature_remove['Feature_remove'][i])\n",
        "\n",
        "feature_remove = new_feature_remove[~new_feature_remove['Feature_remove'].isin(drop_features)]\n",
        "feature_remove.reset_index(drop=True, inplace=True)\n",
        "\n",
        "feature_column = feature_remove.sum(axis=0).values\n",
        "feature_column[0] = 'Total'\n",
        "feature_remove.loc[len(feature_remove)] = feature_column\n",
        "\n",
        "data = data.drop(columns=drop_features, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NDkgT-NAjYiY",
      "metadata": {
        "id": "NDkgT-NAjYiY"
      },
      "source": [
        "### Feature selection methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fMjerzLuiXb_",
      "metadata": {
        "id": "fMjerzLuiXb_"
      },
      "outputs": [],
      "source": [
        "def get_important_features_GRF(data):\n",
        "    # Split data into features and target\n",
        "    X = data.drop(columns=['adl'])\n",
        "    y = data['adl']\n",
        "\n",
        "    # Define parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "        'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
        "        'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split an internal node\n",
        "    }\n",
        "\n",
        "    # Initialize Random Forest classifier\n",
        "    clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Initialize GridSearchCV\n",
        "    grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "    # Train the classifier\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    # Get the best estimator from GridSearchCV\n",
        "    best_clf = grid_search.best_estimator_\n",
        "\n",
        "    # Get feature importances\n",
        "    feature_importances = best_clf.feature_importances_\n",
        "\n",
        "    #Discard irrelevant features.\n",
        "    model = SelectFromModel(best_clf, prefit=True)\n",
        "    X_new = model.transform(X)\n",
        "\n",
        "    #Getting top feature names\n",
        "    cols_idxs = model.get_support(indices=True)\n",
        "    top_features = X.iloc[:,cols_idxs].columns\n",
        "\n",
        "    #Getting feature importance of the top features.\n",
        "    feature_importance_dict = {}\n",
        "    for feature in top_features:\n",
        "      id = X.columns.get_loc(feature)\n",
        "      feature_importance_dict[feature] = feature_importances[id]\n",
        "\n",
        "    feature_importance_dict = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    if len(feature_importance_dict) % 2 != 0:\n",
        "      feature_importance_dict.popitem()\n",
        "\n",
        "    x_labels = list(feature_importance_dict.keys())\n",
        "    y_values = list(feature_importance_dict.values())\n",
        "\n",
        "    # Plot the data using a bar plot\n",
        "    plt.figure(figsize=(8, 6))  # Set figure size\n",
        "    plt.bar(x_labels, y_values, color='skyblue')  # Create a bar plot\n",
        "\n",
        "    # Customize the plot\n",
        "    plt.title('Feature importance MDI')\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('Mean decrease impurity')\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_ticklabels([])\n",
        "\n",
        "    return x_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r_q5wANxkCN6",
      "metadata": {
        "id": "r_q5wANxkCN6"
      },
      "source": [
        "Finding top features to use based on the remaining features \\\n",
        "**Change variable `feature_selection_method` based on which feature selection method to use.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4qeClNtEkgXc",
      "metadata": {
        "id": "4qeClNtEkgXc"
      },
      "outputs": [],
      "source": [
        "important_features = get_important_features_GRF(data.iloc[:,:-3])\n",
        "\n",
        "print(important_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGMPIQgsmlki",
      "metadata": {
        "id": "VGMPIQgsmlki"
      },
      "source": [
        "### Only use the top features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EvPECayCmR-j",
      "metadata": {
        "id": "EvPECayCmR-j"
      },
      "outputs": [],
      "source": [
        "columns = important_features\n",
        "columns.extend(['adl', 'session', 'subject', 'fileID'])\n",
        "data = data[columns]\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8635af",
      "metadata": {
        "id": "5e8635af"
      },
      "source": [
        "## Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60eaf3db",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:21:22.791993Z",
          "iopub.status.busy": "2024-03-06T11:21:22.791637Z",
          "iopub.status.idle": "2024-03-06T11:21:22.800386Z",
          "shell.execute_reply": "2024-03-06T11:21:22.799244Z",
          "shell.execute_reply.started": "2024-03-06T11:21:22.791962Z"
        },
        "id": "60eaf3db"
      },
      "outputs": [],
      "source": [
        "def display_distribution(data,column,fs,w):\n",
        "    plt.figure(figsize=(16, 5))  # Adjust the figure size as per your preference\n",
        "    counts = data[column].value_counts().sort_index()\n",
        "    counts.plot(kind='bar')\n",
        "\n",
        "    for i, v in enumerate(counts):\n",
        "        plt.text(i, v, str(v) + '(' + str(int(round((v/len(data))*100,0)))+'%' + ')', ha='center', va='bottom')\n",
        "\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Distribution of ' + column + ' samples in terms of segments of '+ str(w) + ' second')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e51eef",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:21:22.802390Z",
          "iopub.status.busy": "2024-03-06T11:21:22.801954Z",
          "iopub.status.idle": "2024-03-06T11:21:23.335164Z",
          "shell.execute_reply": "2024-03-06T11:21:23.334284Z",
          "shell.execute_reply.started": "2024-03-06T11:21:22.802360Z"
        },
        "id": "71e51eef"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'adl',FS,w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc89af4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:21:23.337679Z",
          "iopub.status.busy": "2024-03-06T11:21:23.336612Z",
          "iopub.status.idle": "2024-03-06T11:21:23.944576Z",
          "shell.execute_reply": "2024-03-06T11:21:23.942928Z",
          "shell.execute_reply.started": "2024-03-06T11:21:23.337638Z"
        },
        "id": "efc89af4"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'subject',FS, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8dee02d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:21:23.946343Z",
          "iopub.status.busy": "2024-03-06T11:21:23.945980Z",
          "iopub.status.idle": "2024-03-06T11:21:24.223521Z",
          "shell.execute_reply": "2024-03-06T11:21:24.222344Z",
          "shell.execute_reply.started": "2024-03-06T11:21:23.946311Z"
        },
        "id": "d8dee02d"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'session',FS, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bnzakdw4g6It",
      "metadata": {
        "id": "bnzakdw4g6It"
      },
      "source": [
        "## Undersample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y4LxPgc0g9kS",
      "metadata": {
        "id": "y4LxPgc0g9kS"
      },
      "outputs": [],
      "source": [
        "adl_distribution = data['adl'].value_counts().sort_index()\n",
        "adl_smallest_value = adl_distribution.min()\n",
        "\n",
        "rus = RandomUnderSampler(sampling_strategy='not minority', random_state=42)\n",
        "X = data.drop(columns=['adl'])\n",
        "y = data['adl']\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "\n",
        "X_resampled['adl'] = y_resampled\n",
        "data = X_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XQf24ZUnhApG",
      "metadata": {
        "id": "XQf24ZUnhApG"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'adl',FS,w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SX0ZUBt8hCon",
      "metadata": {
        "id": "SX0ZUBt8hCon"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'subject',FS,w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eyuQYgaShEvJ",
      "metadata": {
        "id": "eyuQYgaShEvJ"
      },
      "outputs": [],
      "source": [
        "display_distribution(data, 'session',FS, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b09e3f",
      "metadata": {
        "id": "e7b09e3f"
      },
      "source": [
        "### Rename activities from 0 to N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a531c31b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:26:42.836016Z",
          "iopub.status.busy": "2024-03-06T11:26:42.835231Z",
          "iopub.status.idle": "2024-03-06T11:26:42.853279Z",
          "shell.execute_reply": "2024-03-06T11:26:42.852056Z",
          "shell.execute_reply.started": "2024-03-06T11:26:42.835986Z"
        },
        "id": "a531c31b"
      },
      "outputs": [],
      "source": [
        "unique_values = data['adl'].unique()\n",
        "codes, unique_labels = pd.factorize(unique_values)\n",
        "value_mapping = dict(zip(unique_values, codes))\n",
        "data['adl'] = data['adl'].map(value_mapping)\n",
        "old_new_mapping = dict(zip(unique_values, unique_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e715ee69",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:26:42.715784Z",
          "iopub.status.busy": "2024-03-06T11:26:42.715362Z",
          "iopub.status.idle": "2024-03-06T11:26:42.824793Z",
          "shell.execute_reply": "2024-03-06T11:26:42.823508Z",
          "shell.execute_reply.started": "2024-03-06T11:26:42.715744Z"
        },
        "id": "e715ee69"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix( y_test, y_pred):\n",
        "    classification_report_df = pd.DataFrame(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1),output_dict=True)).transpose()\n",
        "    new_mapping = sorted(value_mapping.keys())\n",
        "    new_mapping.extend(['accuracy', 'macro avg', 'weighted avg'])\n",
        "    classification_report_df.index = new_mapping\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    display(classification_report_df)\n",
        "\n",
        "    cm = np.round(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1),normalize='true')*100,2)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g',\n",
        "                xticklabels=new_mapping[:-3], yticklabels=new_mapping[:-3])\n",
        "\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_validation_history(history):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(8, 2))\n",
        "\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "    axs[0].set_xlabel(\"Epoch\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Loss\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def time_model_inference(model: Model, input_data):\n",
        "    start_time = time.time()\n",
        "    model.predict(input_data)\n",
        "    end_time = time.time()\n",
        "    print('Elapsed Time ', end_time - start_time)\n",
        "    print('# of Test samples', len(input_data))\n",
        "    print('Average Inference time (ms):', (end_time - start_time)/len(input_data)*1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02db8727",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:26:42.855646Z",
          "iopub.status.busy": "2024-03-06T11:26:42.855163Z",
          "iopub.status.idle": "2024-03-06T11:26:42.929010Z",
          "shell.execute_reply": "2024-03-06T11:26:42.928116Z",
          "shell.execute_reply.started": "2024-03-06T11:26:42.855561Z"
        },
        "id": "02db8727"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_the_model(data):\n",
        "    X = data.iloc[0:,0:-4]\n",
        "    y = data['adl']\n",
        "\n",
        "    subject = data['subject'].values\n",
        "    session = data['session'].values\n",
        "    adl = data['adl'].values\n",
        "    adl_encoder = LabelEncoder()\n",
        "    adl = adl_encoder.fit_transform(adl)\n",
        "\n",
        "    if FLAG_SPLIT == 2:\n",
        "        train_session, test_session = train_test_split(np.unique(session), test_size=0.2, random_state=28)\n",
        "        train_session, val_session = train_test_split(train_session, test_size=0.2, random_state=28)\n",
        "\n",
        "        train_idx = np.where(np.isin(session, train_session))[0]\n",
        "        val_idx = np.where(np.isin(session, val_session))[0]\n",
        "        test_idx = np.where(np.isin(session, test_session))[0]\n",
        "\n",
        "        print('Train Sessions: ', train_session)\n",
        "        print('Validation Sessions: ', val_session)\n",
        "        print('Test Sessions: ', test_session)\n",
        "\n",
        "        X_train, X_val, X_test = X.loc[train_idx], X.loc[val_idx], X.loc[test_idx]\n",
        "        y_train, y_val, y_test = y.loc[train_idx], y.loc[val_idx], y.loc[test_idx]\n",
        "\n",
        "    elif FLAG_SPLIT == 1:\n",
        "        train_subjects, test_subjects = train_test_split(np.unique(subject), test_size=0.2, random_state=28)\n",
        "        train_subjects, val_subjects = train_test_split(train_subjects, test_size=0.25, random_state=28)\n",
        "\n",
        "        train_idx = np.where(np.isin(subject, train_subjects))[0]\n",
        "        val_idx = np.where(np.isin(subject, val_subjects))[0]\n",
        "        test_idx = np.where(np.isin(subject, test_subjects))[0]\n",
        "\n",
        "        print(\"Train Subjects :\", train_subjects)\n",
        "        print(\"Validation Subjects :\", val_subjects)\n",
        "        print(\"Test Subjects :\", test_subjects)\n",
        "\n",
        "        X_train, X_val, X_test = X.loc[train_idx], X.loc[val_idx], X.loc[test_idx]\n",
        "        y_train, y_val, y_test = y.loc[train_idx], y.loc[val_idx], y.loc[test_idx]\n",
        "\n",
        "    elif FLAG_SPLIT == 0:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
        "\n",
        "    X_train = X_train.values\n",
        "    X_val = X_val.values\n",
        "    X_test = X_test.values\n",
        "\n",
        "    num_classes = len(adl_encoder.classes_)\n",
        "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "    y_val = to_categorical(y_val, num_classes=num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "\n",
        "    input_shape = (X_train[0].shape)\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], *input_shape)\n",
        "    X_val = X_val.reshape(X_val.shape[0], *input_shape)\n",
        "    X_test = X_test.reshape(X_test.shape[0], *input_shape)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, num_classes, input_shape\n",
        "\n",
        "\n",
        "def evaluate_my_model(model, history, X_test, y_test):\n",
        "    plot_training_validation_history(history)\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "    y_pred =  np.round(model.predict(X_test, batch_size=None, verbose=\"auto\", steps=None, callbacks=None))\n",
        "    plot_confusion_matrix(y_test, y_pred)\n",
        "    time_model_inference(model,X_test)\n",
        "\n",
        "def my_LSTM_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=(X_train[0].shape)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_CNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train[0].shape), padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(256, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_CNNRNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=256, kernel_size=3, activation=\"relu\", input_shape=(X_train[0].shape)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(256)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_RNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(128, input_shape=(X_train[0].shape), return_sequences=True))\n",
        "    model.add(SimpleRNN(128, return_sequences=True))\n",
        "    model.add(SimpleRNN(128, return_sequences=True))\n",
        "    model.add(SimpleRNN(128))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_MLP_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    flatten_layer = Flatten()(input_layer)\n",
        "    hidden_layer1 = Dense(32, activation='relu')(flatten_layer)\n",
        "    dropout1 = Dropout(0.4)(hidden_layer1)\n",
        "    hidden_layer2 = Dense(64, activation='relu')(dropout1)\n",
        "    dropout2 = Dropout(0.4)(hidden_layer2)\n",
        "    hidden_layer3 = Dense(128, activation='relu')(dropout2)\n",
        "    dropout3 = Dropout(0.4)(hidden_layer3)\n",
        "    hidden_layer4 = Dense(256, activation='relu')(dropout3)\n",
        "    dropout4 = Dropout(0.4)(hidden_layer4)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dropout4)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)\n",
        "\n",
        "def my_LSTMBiAtt_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs, batch_size, lr):\n",
        "    input_layer = Input(shape=(X_train[0].shape))\n",
        "    lstm_layer = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
        "    attention = Dense(1, activation='tanh')(lstm_layer)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(256)(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "    attention_mul = Multiply()([lstm_layer, attention])\n",
        "    output_layer = LSTM(128)(attention_mul)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(output_layer)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    pr_metric = AUC(curve='PR', num_thresholds=100)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[pr_metric,\"accuracy\"])\n",
        "    stop_early=EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto',restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_val,y_val),callbacks=[stop_early])\n",
        "    evaluate_my_model(model, history, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa67f9d",
      "metadata": {
        "id": "1aa67f9d"
      },
      "source": [
        "## Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0bcc22",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:26:55.254754Z",
          "iopub.status.busy": "2024-03-06T11:26:55.254275Z",
          "iopub.status.idle": "2024-03-06T11:26:55.618735Z",
          "shell.execute_reply": "2024-03-06T11:26:55.617379Z",
          "shell.execute_reply.started": "2024-03-06T11:26:55.254716Z"
        },
        "id": "4d0bcc22"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, num_classes, input_shape = prepare_data_for_the_model(data)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\",  X_test.shape)\n",
        "\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"y_test shape:\",  y_test.shape)\n",
        "\n",
        "print(\"Number of classes:\", num_classes)\n",
        "print(\"Input shape:\", input_shape)\n",
        "\n",
        "X_train = X_train.reshape(len(X_train),-1,2)\n",
        "X_val = X_val.reshape(len(X_val),-1,2)\n",
        "X_test = X_test.reshape(len(X_test),-1,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ef0b64",
      "metadata": {
        "id": "19ef0b64"
      },
      "source": [
        "## Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4879e3d8",
      "metadata": {
        "id": "4879e3d8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "my_CNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e271c9e1",
      "metadata": {
        "id": "e271c9e1"
      },
      "outputs": [],
      "source": [
        "my_CNNRNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c692099",
      "metadata": {
        "id": "0c692099"
      },
      "outputs": [],
      "source": [
        "my_LSTM_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc3b945",
      "metadata": {
        "id": "2fc3b945"
      },
      "outputs": [],
      "source": [
        "my_RNN_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cddc02f",
      "metadata": {
        "id": "1cddc02f"
      },
      "outputs": [],
      "source": [
        "my_MLP_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9809a2c4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T11:27:08.854310Z",
          "iopub.status.busy": "2024-03-06T11:27:08.853916Z"
        },
        "id": "9809a2c4"
      },
      "outputs": [],
      "source": [
        "my_LSTMBiAtt_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, EPOCHS, BATCH_SIZE, LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3D3tSBvAaXnN",
      "metadata": {
        "id": "3D3tSBvAaXnN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4450820,
          "sourceId": 7637398,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
