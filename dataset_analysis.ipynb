{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-uEhmFIW_IP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoXujD-0nD9H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcajStc8W_IS"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "`FILE_PATH`: file containing the data. \\\n",
        "`FS`: the hertz used in the file. \\\n",
        "`ADL_TITLE`: The activity that were done. The index is the activity ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCQ6RL2IW_IT"
      },
      "outputs": [],
      "source": [
        "FILE_PATH = '/content/drive/My Drive/MASTER/RAWFILES/UiS4ADL-50hz.csv'\n",
        "FS = 50\n",
        "ADL_TITLE = [\"No activity\",\"Drink water\", \"Eat meal\", \"Open a bottle\", \"Open a box\", \"Brush teeth\", \"Brush hair\",\n",
        "             \"Take off a jacket\", \"Put on a jacket\", \"Put on a shoe\", \"Take off a shoe\", \"Put on glasses\", \"Take off glasses\",\n",
        "             \"Sit down\", \"Stand up\", \"Writing\", \"Phone call\", \"Type on a keyboard\", \"Salute (wave hand)\",\n",
        "             \"Sneeze cough\", \"Blow nose\", \"Washing hands\", \"Dusting\", \"Ironing\", \"Washing dishes\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwSccLQfW_IT"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1AF297LW_IU"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shg_fpW9W_IU"
      },
      "source": [
        "### Data to drop\n",
        "Dropping subjects data because of incorrect data recording. \\\n",
        "**Change variable `subjects_to_drop` accordingly to goal.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDDrKlDxW_IU"
      },
      "outputs": [],
      "source": [
        "subjects_to_drop = [1727,1826,2097]\n",
        "to_drop = data[data['subject'].isin(subjects_to_drop)]\n",
        "data.drop(to_drop.index, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlt0jnGaW_IU"
      },
      "source": [
        "### Analysis if there's missing data, and drop it if there's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl5sQqgVW_IU"
      },
      "outputs": [],
      "source": [
        "to_drop = data[data.isna().any(axis=1)]\n",
        "print(to_drop.subject.unique(),to_drop.session.unique(),to_drop.adl.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ204JAeW_IU"
      },
      "outputs": [],
      "source": [
        "data.drop(to_drop.index,inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upbktl-rW_IV"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3BvrZInW_IV"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\n",
        "sns.heatmap(data.T.isna(), cmap='Blues')\n",
        "ax.set_title('Fields with Missing Values', fontsize=16)\n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey6wx_GiW_IV"
      },
      "source": [
        "## Overview of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS0m5bPaW_IV"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset Overview:\")\n",
        "display((data.head()))\n",
        "print()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Values:\")\n",
        "print(data.isnull().sum())\n",
        "print()\n",
        "\n",
        "# Basic statistics of the numerical columns\n",
        "print(\"Basic Statistics:\")\n",
        "display(data.describe().iloc[0:,1:-4])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBfHg5jOW_IV"
      },
      "source": [
        "### Distribution information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7nHCnicSKw1"
      },
      "outputs": [],
      "source": [
        "number_samples = []\n",
        "adl_id = []\n",
        "\n",
        "for i in range(len(ADL_TITLE)):\n",
        "  number_samples.append(data[data['adl'] == i].count().unique()[0])\n",
        "  adl_id.append(i)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(30, 20))\n",
        "ax.bar(adl_id, number_samples)\n",
        "ax.set_title(\"Total ADLs samples\", {'size': 25})\n",
        "ax.set_xlabel(\"ADL\", {'size': 25})\n",
        "ax.set_ylabel(\"Samples\", {'size': 25})\n",
        "ax.xaxis.set_tick_params(labelsize=25)\n",
        "ax.yaxis.set_tick_params(labelsize=25)\n",
        "ax.set_xticks(range(0,len(ADL_TITLE)))\n",
        "\n",
        "for bar in ax.patches:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom', rotation=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq_YfmZwfO5c"
      },
      "outputs": [],
      "source": [
        "distribution_seconds = np.sum(np.array(number_samples)/FS)\n",
        "print(\"Total ADLs samples in seconds: \" + str(distribution_seconds) + \" seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xMVpVX1xrtK"
      },
      "outputs": [],
      "source": [
        "number_subjects = []\n",
        "session_count = []\n",
        "session_for_adl = [5,5,3,5,5,5,5,3,3,3,3,5,5,5,5,5,5,5,5,5,5,5,5,5,5]\n",
        "\n",
        "\n",
        "for i in range(len(ADL_TITLE)):\n",
        "  adl_selector = data[data['adl'] == i]\n",
        "  number_subjects.append(len(adl_selector.subject.unique()))\n",
        "  session_count.append(len(adl_selector.fileID.unique()))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(30, 20))\n",
        "ax[0].bar(adl_id, number_subjects)\n",
        "ax[0].set_title(\"Unique subjects performing ADLs\", {'size': 25})\n",
        "ax[0].set_xlabel(\"ADL\", {'size': 25})\n",
        "ax[0].set_ylabel(\"Unique subjects count\", {'size': 25})\n",
        "ax[0].xaxis.set_tick_params(labelsize=25)\n",
        "ax[0].yaxis.set_tick_params(labelsize=25)\n",
        "ax[0].set_xticks(range(0,len(ADL_TITLE)))\n",
        "for i in range(len(number_subjects)):\n",
        "  ax[0].hlines(y=len(data.subject.unique()), xmin=i-0.4, xmax=i+0.4, linewidth=2, color='r')\n",
        "\n",
        "for bar in ax[0].patches:\n",
        "    height = bar.get_height()\n",
        "    ax[0].text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "ax[1].bar(adl_id, session_count)\n",
        "ax[1].set_title(\"Total ADLs sessions\", {'size': 25})\n",
        "ax[1].set_xlabel(\"ADL\", {'size': 25})\n",
        "ax[1].set_ylabel(\"Sessions count\", {'size': 25})\n",
        "ax[1].xaxis.set_tick_params(labelsize=25)\n",
        "ax[1].yaxis.set_tick_params(labelsize=25)\n",
        "ax[1].set_xticks(range(0,len(ADL_TITLE)))\n",
        "for i in range(len(number_subjects)):\n",
        "  if i == 0:\n",
        "    continue\n",
        "  ax[1].hlines(y=number_subjects[i]*session_for_adl[i], xmin=i-0.4, xmax=i+0.4, linewidth=2, color='r')\n",
        "\n",
        "for bar in ax[1].patches:\n",
        "    height = bar.get_height()\n",
        "    ax[1].text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAEkajymbpMj"
      },
      "outputs": [],
      "source": [
        "session_samples = []\n",
        "session_id = []\n",
        "\n",
        "for s in data.session.unique():\n",
        "  session_samples.append(len(data[data['session'] == s]))\n",
        "  session_id.append(s)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(30, 20))\n",
        "ax.bar(session_id, session_samples)\n",
        "ax.set_title(\"Total sessions samples\", {'size': 25})\n",
        "ax.set_xlabel(\"Session\", {'size': 25})\n",
        "ax.set_ylabel(\"Samples\", {'size': 25})\n",
        "ax.xaxis.set_tick_params(labelsize=25)\n",
        "ax.yaxis.set_tick_params(labelsize=25)\n",
        "\n",
        "for bar in ax.patches:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_yqdcmLgPYM"
      },
      "outputs": [],
      "source": [
        "number_subjects = []\n",
        "total_session_count = []\n",
        "session_id = []\n",
        "\n",
        "for s in data.session.unique():\n",
        "  session_selector = data[data['session'] == s]\n",
        "  total_session_count.append(len(session_selector.fileID.unique()))\n",
        "  number_subjects.append(len(session_selector.subject.unique()))\n",
        "  session_id.append(s)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(30, 20))\n",
        "ax[0].bar(session_id, number_subjects)\n",
        "ax[0].set_title(\"Unique subjects performing sessions\", {'size': 25})\n",
        "ax[0].set_xlabel(\"Session\", {'size': 25})\n",
        "ax[0].set_ylabel(\"Unique subjects count\", {'size': 25})\n",
        "ax[0].xaxis.set_tick_params(labelsize=25)\n",
        "ax[0].yaxis.set_tick_params(labelsize=25)\n",
        "\n",
        "for bar in ax[0].patches:\n",
        "    height = bar.get_height()\n",
        "    ax[0].text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "ax[1].bar(session_id, total_session_count)\n",
        "ax[1].set_title(\"Total sessions\", {'size': 25})\n",
        "ax[1].set_xlabel(\"Session\", {'size': 25})\n",
        "ax[1].set_ylabel(\"Sessions count\", {'size': 25})\n",
        "ax[1].xaxis.set_tick_params(labelsize=25)\n",
        "ax[1].yaxis.set_tick_params(labelsize=25)\n",
        "\n",
        "for bar in ax[1].patches:\n",
        "    height = bar.get_height()\n",
        "    ax[1].text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSBsGmH1jFzB"
      },
      "outputs": [],
      "source": [
        "subject_samples = []\n",
        "subject_id = []\n",
        "\n",
        "for s in data.subject.unique():\n",
        "  subject_samples.append(len(data[data['subject'] == s]))\n",
        "  subject_id.append(str(s))\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(30, 20))\n",
        "ax.bar(subject_id, subject_samples)\n",
        "ax.set_title(\"Total subject samples\", {'size': 25})\n",
        "ax.set_xlabel(\"Subject\", {'size': 25})\n",
        "ax.set_ylabel(\"Samples\", {'size': 25})\n",
        "ax.xaxis.set_tick_params(labelsize=25, rotation=270)\n",
        "ax.yaxis.set_tick_params(labelsize=25)\n",
        "\n",
        "for bar in ax.patches:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9jvoN1cksHn"
      },
      "outputs": [],
      "source": [
        "subject_ADL = []\n",
        "subject_sessions = []\n",
        "subject_id = []\n",
        "\n",
        "for s in data.subject.unique():\n",
        "  subject_selector = data[data['subject'] == s]\n",
        "  subject_ADL.append(len(subject_selector.adl.unique()))\n",
        "  subject_sessions.append(len(subject_selector.fileID.unique()))\n",
        "  subject_id.append(str(s))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(30, 30))\n",
        "ax[0].bar(subject_id, subject_ADL)\n",
        "ax[0].set_title(\"Total ADLs performed by subjects\", {'size': 25})\n",
        "ax[0].set_xlabel(\"Subject\", {'size': 25})\n",
        "ax[0].set_ylabel(\"ADLs count\", {'size': 25})\n",
        "ax[0].xaxis.set_tick_params(labelsize=25, rotation=270)\n",
        "ax[0].yaxis.set_tick_params(labelsize=25)\n",
        "\n",
        "for bar in ax[0].patches:\n",
        "    height = bar.get_height()\n",
        "    ax[0].text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "ax[1].bar(subject_id, subject_sessions)\n",
        "ax[1].set_title(\"Total sessions performed by subjects\", {'size': 25})\n",
        "ax[1].set_xlabel(\"Session\", {'size': 25})\n",
        "ax[1].set_ylabel(\"Sessions count\", {'size': 25})\n",
        "ax[1].xaxis.set_tick_params(labelsize=25, rotation=270)\n",
        "ax[1].yaxis.set_tick_params(labelsize=25)\n",
        "\n",
        "for bar in ax[1].patches:\n",
        "    height = bar.get_height()\n",
        "    ax[1].text(bar.get_x() + bar.get_width() / 2.0, height, f' {height:.0f}', fontsize=20,\n",
        "            ha='center', va='bottom')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxINtiJJW_IW"
      },
      "source": [
        "### ADL visualization plots\n",
        "**Change variable `columns` and `rows` accordingly to how the plot layout should be.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dZnM1ZqW_IW"
      },
      "outputs": [],
      "source": [
        "correlation_dataframe = pd.DataFrame()\n",
        "\n",
        "rows = 5\n",
        "columns = 5\n",
        "row = 0\n",
        "fig, ax = plt.subplots(rows, columns, figsize=(30, 20))\n",
        "for i in range(len(ADL_TITLE)):\n",
        "    if i%columns == 0:\n",
        "        if i > columns-1:\n",
        "            row += 1\n",
        "\n",
        "    top_features_mean = data[data['adl'] == i].mean()\n",
        "    top_features_mean = top_features_mean[top_features_mean.index[1:-4]]\n",
        "\n",
        "    sns.lineplot(y=top_features_mean, x=top_features_mean.index, ax=ax[row][i%columns])\n",
        "    correlation_dataframe[i] = top_features_mean\n",
        "    ax[row][i%columns].set_title(ADL_TITLE[i])\n",
        "    ax[row][i%columns].set_xticklabels([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzv-AmenW_IW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(26, 22))\n",
        "sns.heatmap(correlation_dataframe.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('ADL Correlation Heatmap')\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft_T12HgW_IX"
      },
      "source": [
        "### Dataset Distrubution Study x Session (in seconds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bitzaZEVW_IX"
      },
      "outputs": [],
      "source": [
        "table_1 = pd.crosstab(data[\"adl\"],data[\"session\"])/FS\n",
        "table_1['Total'] = table_1.sum(axis=1)\n",
        "table_1.loc['Total'] = table_1.sum(axis=0).values\n",
        "table_1 = table_1.astype('float').round(2)\n",
        "\n",
        "table_1.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G25CyCWhW_IX"
      },
      "outputs": [],
      "source": [
        "table_1.T.iloc[:-1,:-1].describe().iloc[1:,0:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk7YKbWYW_IX"
      },
      "outputs": [],
      "source": [
        "table_1.iloc[:-1,:-1].describe().iloc[1:,0:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs01MRr1W_IX"
      },
      "source": [
        "### Dataset Distrubution Study x Subject (in seconds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mqNN1F9W_IX"
      },
      "outputs": [],
      "source": [
        "table_1 = pd.crosstab(data[\"adl\"],data[\"subject\"])/FS\n",
        "table_1['Total'] = table_1.sum(axis=1)\n",
        "table_1.loc['Total'] = table_1.sum(axis=0).values\n",
        "table_1 = table_1.astype('float').round(2)\n",
        "table_1.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKfpdaN3W_IY"
      },
      "outputs": [],
      "source": [
        "table_1.T.iloc[:-1,:-1].describe().iloc[1:,0:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHjlQOhkW_IZ"
      },
      "outputs": [],
      "source": [
        "table_1.iloc[:-1,:-1].describe().iloc[1:,0:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYMDbVvTW_IZ"
      },
      "source": [
        "### Dataset Distrubution Study x Subject and Session (in seconds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzTlZ200W_IZ"
      },
      "outputs": [],
      "source": [
        "table_1 = pd.crosstab(data[\"subject\"],data[\"session\"])/FS\n",
        "table_1['Total'] = table_1.sum(axis=1)\n",
        "table_1.loc['Total'] = table_1.sum(axis=0).values\n",
        "table_1 = table_1.astype('float').round(2)\n",
        "table_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDgkc5LAW_IZ"
      },
      "outputs": [],
      "source": [
        "table_1.T.iloc[:-1,:-1].describe().iloc[1:,0:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeotwTGeW_IZ"
      },
      "outputs": [],
      "source": [
        "table_1.iloc[:-1,:-1].describe().iloc[1:,0:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4iTD_U6W_IZ"
      },
      "source": [
        "### Specific ADL and session analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzP5L7UCW_Ia"
      },
      "outputs": [],
      "source": [
        "data_series = data.iloc[0:,-4:][~(data.iloc[0:,-4:].shift() == data.iloc[0:,-4:]).all(axis=1)]\n",
        "display(data_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRgnf6TwW_Ia"
      },
      "outputs": [],
      "source": [
        "#Adding samples and duration to dataframe\n",
        "stats_samples = []\n",
        "for i in range(0,data_series.shape[0]-1):\n",
        "    stats_samples.append(int(data_series.index[i+1]-data_series.index[i]))\n",
        "\n",
        "stats_samples.append(0)\n",
        "data_series['Samples'] = stats_samples\n",
        "data_series['Duration (sec)'] = data_series['Samples']/FS\n",
        "\n",
        "display(data_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw2wZw35LW6c"
      },
      "source": [
        "### Outliers using IQR\n",
        "Outliers in percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fJtIBS7LbA_"
      },
      "outputs": [],
      "source": [
        "for i in range(len(ADL_TITLE)):\n",
        "  adl_selector = data_series[data_series['adl'] == i]\n",
        "  Q1 = adl_selector['Duration (sec)'].quantile(0.25)\n",
        "  Q3 = adl_selector['Duration (sec)'].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  threshold = 1.5\n",
        "  outliers = adl_selector[(adl_selector['Duration (sec)'] < Q1 - threshold * IQR) | (adl_selector['Duration (sec)'] > Q3 + threshold * IQR)]\n",
        "\n",
        "  print('ADL ' + str(i) + \": \" + str(round(len(outliers)/len(adl_selector),4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kCE6cObSdKB"
      },
      "source": [
        "ADL specific outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BViWPcwsSiFK"
      },
      "outputs": [],
      "source": [
        "adl_selector = data_series[data_series['adl'] == 12]\n",
        "Q1 = adl_selector['Duration (sec)'].quantile(0.25)\n",
        "Q3 = adl_selector['Duration (sec)'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "threshold = 1.5\n",
        "outliers = adl_selector[(adl_selector['Duration (sec)'] < Q1 - threshold * IQR) | (adl_selector['Duration (sec)'] > Q3 + threshold * IQR)]\n",
        "\n",
        "display(outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLmDVaxW_Ia"
      },
      "source": [
        "## Find window size and group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7NLX1x0W_Ia"
      },
      "outputs": [],
      "source": [
        "adls = ['A'+str(i) for i in range(0,len(ADL_TITLE))]\n",
        "adls_stats = {}\n",
        "for adl_id in data_series.adl.unique():\n",
        "    tmp = data_series[data_series.adl==adl_id]\n",
        "    skew = tmp.skew().iloc[5]\n",
        "    tmp = tmp.describe().iloc[1:,5]\n",
        "    tmp.columns = adl_id\n",
        "    tmp['skew'] = skew\n",
        "    adls_stats[adl_id] = tmp\n",
        "\n",
        "sorted_adls_stats = dict(sorted(adls_stats.items()))\n",
        "adls_stats = [pd.DataFrame(sorted_adls_stats[key]) for key in sorted_adls_stats]\n",
        "adls_stats = pd.concat(adls_stats, axis=1)\n",
        "adls_stats = adls_stats.set_axis(adls, axis=1)\n",
        "adls_stats = adls_stats.round(2)\n",
        "adls_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW4xxzO9W_Ib"
      },
      "source": [
        "**Change variable `columns` and `rows` accordingly to how the plot layout should be.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnwYwUhNW_Ib"
      },
      "outputs": [],
      "source": [
        "rows = 5\n",
        "columns = 5\n",
        "row = 0\n",
        "\n",
        "fig, ax = plt.subplots(rows, columns, figsize=(20, 20))\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "for adls in range(0,len(ADL_TITLE)):\n",
        "    if adls%columns == 0:\n",
        "        if adls > columns-1:\n",
        "            row += 1\n",
        "    sns.lineplot(y = list(adls_stats.iloc[:,adls]), x = ['mean', 'std', 'min', '25%', '50%', '75%', 'max', 'skew'], ax=ax[row][adls%columns], marker='o')\n",
        "    ax[row][adls%columns].set_title(\"adl \" + str(adls))\n",
        "    ax[row][adls%columns].set_ylim(0, max(list(adls_stats.iloc[:,adls])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osatfGtkW_Ib"
      },
      "source": [
        "**Change variable `columns` and `rows` accordingly to how the plot layout should be.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y-sL0y2W_Ib"
      },
      "outputs": [],
      "source": [
        "rows = 5\n",
        "columns = 5\n",
        "row = 0\n",
        "\n",
        "fig, ax = plt.subplots(rows, columns, figsize=(20, 20))\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "for adls in range(0,len(ADL_TITLE)):\n",
        "    if adls%columns == 0:\n",
        "        if adls > columns-1:\n",
        "            row += 1\n",
        "    statistics = adls_stats.iloc[:,adls]\n",
        "\n",
        "    sns.histplot(data = data_series[data_series['adl'] == adls], x = \"Duration (sec)\", ax=ax[row][adls%columns])\n",
        "    ax[row][adls%columns].axvline(x=statistics[3], color='r', linestyle='--')\n",
        "    ax[row][adls%columns].axvline(x=statistics[4], color='b', linestyle='--')\n",
        "    ax[row][adls%columns].axvline(x=statistics[5], color='g', linestyle='--')\n",
        "\n",
        "    red_legend = mpatches.Patch(color='red', label='25%')\n",
        "    blue_legend = mpatches.Patch(color='blue', label='50%')\n",
        "    green_legend = mpatches.Patch(color='green', label='75%')\n",
        "\n",
        "    ax[row][adls%columns].legend(handles=[red_legend, blue_legend, green_legend])\n",
        "\n",
        "    ax[row][adls%columns].set_title(\"ADL \" + str(adls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_69SdH7cW_Ib"
      },
      "outputs": [],
      "source": [
        "adls_stats = adls_stats.T\n",
        "adls_stats['25%']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ7Z7n_VW_Ib"
      },
      "outputs": [],
      "source": [
        "display(round(adls_stats['25%'].describe(),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPLX63MbW_Ib"
      },
      "source": [
        "**Change variable `thresholds` and `stat_type` accordingly to goal.**\n",
        "\n",
        "Treshold examples:\n",
        "- [\"5\", \"5-10\", \"10\"]\n",
        "    - threshold <=5 , threshold > 5 < 10, threshold >= 10\n",
        "- [\"5\", \"10\"]\n",
        "    - threshold <=5, threshold >= 10\n",
        "- [\"5\", \"5-7\", \"7-9\", \"10\"]\n",
        "    - threshold <=5, threshold > 5 < 7, threshold > 7 < 9, threshold >= 10\n",
        "\n",
        "Removing adl 0 since not wanting to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLOBBHDwW_Ic"
      },
      "outputs": [],
      "source": [
        "adls_stats.drop('A0', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlrB83_8W_Ic"
      },
      "outputs": [],
      "source": [
        "thresholds = [\"5\", \"5-10\", \"10\"]\n",
        "stat_type = '25%'\n",
        "threshold_results = []\n",
        "\n",
        "if(len(thresholds) > 1):\n",
        "\n",
        "    X = adls_stats.index\n",
        "    X_axis = np.arange(1,len(X)+1)\n",
        "\n",
        "    fig, ax = plt.subplots(1, len(thresholds), figsize=(20, 20))\n",
        "\n",
        "    for i in range(len(thresholds)):\n",
        "\n",
        "        threshold_split = thresholds[i].split(\"-\")\n",
        "\n",
        "        if i == 0:\n",
        "            threshold_result = [1 if x in (adls_stats[adls_stats[stat_type]<=int(thresholds[i])].index) else 0 for x in X]\n",
        "            result_array = (adls_stats[stat_type]*threshold_result)\n",
        "            threshold_results.append(result_array)\n",
        "            ax[i].set_title(\"threshold <= \" + thresholds[i])\n",
        "            ax[i].set_xlabel(\"ADLs\")\n",
        "            ax[i].set_ylabel(\"Duration\")\n",
        "\n",
        "        else:\n",
        "            if len(threshold_split) == 2:\n",
        "\n",
        "                first_threshold = int(threshold_split[0])\n",
        "                second_threshold = int(threshold_split[1])\n",
        "\n",
        "                threshold_result = [1 if x in (adls_stats[adls_stats[stat_type].between(first_threshold,second_threshold)].index) else 0 for x in X]\n",
        "                result_array = (adls_stats[stat_type]*threshold_result)\n",
        "                threshold_results.append(result_array)\n",
        "                ax[i].set_title(\"threshold > \" + threshold_split[0] + \" < \" + threshold_split[1])\n",
        "                ax[i].set_xlabel(\"ADLs\")\n",
        "                ax[i].set_ylabel(\"Duration\")\n",
        "\n",
        "            else:\n",
        "\n",
        "                threshold_result = [1 if x in (adls_stats[adls_stats[stat_type]>=int(thresholds[i])].index) else 0 for x in X]\n",
        "                result_array = (adls_stats[stat_type]*threshold_result)\n",
        "                threshold_results.append(result_array)\n",
        "                ax[i].set_title(\"threshold >= \" + thresholds[i])\n",
        "                ax[i].set_xlabel(\"ADLs\")\n",
        "                ax[i].set_ylabel(\"Duration\")\n",
        "\n",
        "        ax[i].bar(X_axis, result_array, 0.4, label = stat_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNiHb6tQW_Ic"
      },
      "outputs": [],
      "source": [
        "for result in threshold_results:\n",
        "    plotted_result = result[result>0]\n",
        "    display(round(adls_stats[adls_stats.index.isin(plotted_result.index)],2))\n",
        "    print(\"Size: \" + str(len(plotted_result)))\n",
        "    print(plotted_result.index)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHG9Gd2oW_Ic"
      },
      "source": [
        "**Change variable `segment_thresholds` accordingly to goal.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_m3lQmvW_Ic"
      },
      "outputs": [],
      "source": [
        "segment_thresholds = [2, 5, 10]\n",
        "\n",
        "if(len(segment_thresholds) != 0):\n",
        "    for i in range(len(segment_thresholds)):\n",
        "\n",
        "        segment_dataframe = threshold_results[i]\n",
        "        segment_dataframe = segment_dataframe[segment_dataframe > 0]\n",
        "\n",
        "        print(\"threshold < \" + str(segment_thresholds[i]))\n",
        "        adls_in_threshold = [int(label[1:]) for label in segment_dataframe.index]\n",
        "        adls_dataframe = data_series[data_series['adl'].isin(adls_in_threshold)]\n",
        "        val = round(len(adls_dataframe[(adls_dataframe['Duration (sec)']<segment_thresholds[i])])/len(adls_dataframe),4)*100\n",
        "        print(val, \" % of lost segments for all ADL\")\n",
        "\n",
        "        for j in range(len(adls_in_threshold)):\n",
        "            val = round(len(adls_dataframe[(adls_dataframe['adl'] == adls_in_threshold[j]) &\n",
        "                (adls_dataframe['Duration (sec)']<segment_thresholds[i])])/len(adls_dataframe[adls_dataframe['adl'] == adls_in_threshold[j]]),4)*100\n",
        "            print(val, \" % of lost segments for ADL \", adls_in_threshold[j])\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZF1W8EWW_Ic"
      },
      "source": [
        "## Feature selection - correlation and variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDE9h-6OW_Id"
      },
      "source": [
        "**Change variable `groups` accordingly to observations from previous observations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU9wtJyyW_Id"
      },
      "outputs": [],
      "source": [
        "groups = pd.DataFrame({\n",
        "    'windowSize': [2,5,10],\n",
        "    'adl': [[1, 3, 4, 7, 10, 11, 12, 13, 14, 16, 19, 20], [6, 8, 9, 18, 21, 24], [2, 5, 15, 17, 22, 23]]\n",
        "})\n",
        "groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clqmpnbHW_Id"
      },
      "source": [
        "### Feature variance for all ADLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmu18gaCW_Id"
      },
      "outputs": [],
      "source": [
        "variance_features_df = {\n",
        "   \"Feature\": data.columns.to_list()[1:-4]\n",
        "    }\n",
        "variance_features_df = pd.DataFrame(variance_features_df)\n",
        "\n",
        "variance_features = data.iloc[0:,1:-4].var()\n",
        "variance_features_df['Variance'] = variance_features.values\n",
        "sorted_variances = variance_features_df.sort_values('Variance')\n",
        "\n",
        "display(sorted_variances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI4WYEHvW_Ie"
      },
      "source": [
        "### Feature variance based on ADL ID\n",
        "List the feature to remove for each ADL ID based on variance. \\\n",
        "1 is remove, 0 is to keep.\n",
        "**Change variable `variance_threshold` accordingly goal.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilp55XVfW_Ie"
      },
      "outputs": [],
      "source": [
        "variance_threshold = 0.00\n",
        "feature_remove = {\n",
        "   \"Feature_remove\": data.columns.to_list()[1:-4]\n",
        "    }\n",
        "feature_remove = pd.DataFrame(feature_remove)\n",
        "\n",
        "for i in range(1,len(ADL_TITLE)):\n",
        "    variance_features = data[data['adl'] == i].iloc[0:,1:-4].var()\n",
        "    feature_keep_index = variance_features[variance_features > variance_threshold].index.tolist()\n",
        "\n",
        "    result = []\n",
        "    for feature in feature_remove['Feature_remove']:\n",
        "        if feature in feature_keep_index:\n",
        "            result.append(0)\n",
        "        else:\n",
        "            result.append(1)\n",
        "    feature_remove[i] = result\n",
        "\n",
        "feature_remove['Total'] = feature_remove.iloc[0:,1:].sum(axis=1)\n",
        "feature_column = feature_remove.sum(axis=0).values\n",
        "feature_column[0] = 'Total'\n",
        "feature_remove.loc[len(feature_remove)] = feature_column\n",
        "display(feature_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS47XJL0TcJk"
      },
      "source": [
        "Remove the features which have been recognized to have low variance in one of the ADLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nwcmFM0Tg9T"
      },
      "outputs": [],
      "source": [
        "drop_features = []\n",
        "\n",
        "new_feature_remove = feature_remove.iloc[:-1, :]\n",
        "for i in range(len(new_feature_remove['Total'])):\n",
        "  if new_feature_remove['Total'][i] > 0:\n",
        "    drop_features.append(new_feature_remove['Feature_remove'][i])\n",
        "\n",
        "feature_remove = new_feature_remove[~new_feature_remove['Feature_remove'].isin(drop_features)]\n",
        "feature_remove.reset_index(drop=True, inplace=True)\n",
        "\n",
        "feature_column = feature_remove.sum(axis=0).values\n",
        "feature_column[0] = 'Total'\n",
        "feature_remove.loc[len(feature_remove)] = feature_column\n",
        "\n",
        "data = data.drop(columns=drop_features, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-AroJOzW_Ie"
      },
      "source": [
        "### Feature correlation based on ADL ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxjXzLa9W_Ie"
      },
      "source": [
        "The features with highest variance will be checked first for correlation. \\\n",
        "\n",
        "**Change variable `correlation_threshold` accordingly to goal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVEp8phSW_Ie"
      },
      "outputs": [],
      "source": [
        "correlation_threshold = 0.95\n",
        "\n",
        "new_feature_remove = feature_remove.iloc[:-1, :-1]\n",
        "for i in range(1,len(ADL_TITLE)):\n",
        "    feature_columns = data.columns.tolist()[1:-4]\n",
        "\n",
        "    #Finding variance and sort by descending\n",
        "    variance_features_df = {\n",
        "    \"Feature\": feature_columns\n",
        "    }\n",
        "    variance_features_df = pd.DataFrame(variance_features_df)\n",
        "\n",
        "    #Adding adl such that we can select based on adl.\n",
        "    feature_columns.extend([\"adl\"])\n",
        "    data_copy = data[feature_columns]\n",
        "\n",
        "    variance_features = data_copy[data_copy['adl'] == i].var()\n",
        "    variance_features = variance_features.drop(\"adl\")\n",
        "    variance_features_df['Variance'] = variance_features.values\n",
        "\n",
        "    variance_features_df = variance_features_df.sort_values('Variance', ascending=False)\n",
        "    highest_variance_features = variance_features_df['Feature'].tolist()\n",
        "\n",
        "    #Adding adl such that we can select based on adl.\n",
        "    highest_variance_features.extend([\"adl\"])\n",
        "\n",
        "    #Restructure data copy to have same order as highest variance features.\n",
        "    data_copy = data_copy.reindex(columns=highest_variance_features)\n",
        "\n",
        "    correlation_matrix = data_copy[data_copy['adl'] == i].corr()\n",
        "    correlation_matrix = correlation_matrix.iloc[:-1,:-1]\n",
        "    #Remove the high correlation features and only keep one left.\n",
        "    #Keep the one with highest variance.\n",
        "\n",
        "    correlation_features_remove = []\n",
        "    for column in correlation_matrix.columns:\n",
        "        if column in correlation_features_remove:\n",
        "            continue\n",
        "\n",
        "        feature_column = correlation_matrix[column].abs()\n",
        "        selected_features = feature_column[feature_column > correlation_threshold].index.tolist()\n",
        "        filtered_features = [item for item in selected_features if item != column]\n",
        "\n",
        "        for feature in filtered_features:\n",
        "            if feature not in correlation_features_remove:\n",
        "                correlation_features_remove.append(feature)\n",
        "\n",
        "    for feature in correlation_features_remove:\n",
        "        for j in range(len(new_feature_remove['Feature_remove'].values)):\n",
        "            if feature == new_feature_remove['Feature_remove'].values[j]:\n",
        "                new_feature_remove[i][j] = 1\n",
        "                break\n",
        "\n",
        "new_feature_remove['Total'] = new_feature_remove.iloc[0:,1:].sum(axis=1)\n",
        "feature_column = new_feature_remove.sum(axis=0).values\n",
        "feature_column[0] = 'Total'\n",
        "new_feature_remove.loc[len(new_feature_remove)] = feature_column\n",
        "feature_remove = new_feature_remove\n",
        "display(feature_remove)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N4BvqMRyYJd"
      },
      "source": [
        "Remove the features which have been recognized to have low correlation in one of the ADLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHp51IWEyNBa"
      },
      "outputs": [],
      "source": [
        "drop_features = []\n",
        "\n",
        "new_feature_remove = feature_remove.iloc[:-1, :]\n",
        "for i in range(len(new_feature_remove['Total'])):\n",
        "  if new_feature_remove['Total'][i] > 0:\n",
        "    drop_features.append(new_feature_remove['Feature_remove'][i])\n",
        "\n",
        "feature_remove = new_feature_remove[~new_feature_remove['Feature_remove'].isin(drop_features)]\n",
        "feature_remove.reset_index(drop=True, inplace=True)\n",
        "\n",
        "feature_column = feature_remove.sum(axis=0).values\n",
        "feature_column[0] = 'Total'\n",
        "feature_remove.loc[len(feature_remove)] = feature_column\n",
        "\n",
        "data = data.drop(columns=drop_features, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zyrexy5W_If"
      },
      "source": [
        "### Feature selection methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX9nFx24W_If"
      },
      "outputs": [],
      "source": [
        "def get_important_features_GRF(data):\n",
        "    # Split data into features and target\n",
        "    X = data.drop(columns=['adl'])\n",
        "    y = data['adl']\n",
        "\n",
        "    # Define parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "        'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
        "        'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split an internal node\n",
        "    }\n",
        "\n",
        "    # Initialize Random Forest classifier\n",
        "    clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Initialize GridSearchCV\n",
        "    grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "    # Train the classifier\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    # Get the best estimator from GridSearchCV\n",
        "    best_clf = grid_search.best_estimator_\n",
        "\n",
        "    # Get feature importances\n",
        "    feature_importances = best_clf.feature_importances_\n",
        "\n",
        "    #Discard irrelevant features.\n",
        "    model = SelectFromModel(best_clf, prefit=True)\n",
        "    X_new = model.transform(X)\n",
        "\n",
        "    #Getting top feature names\n",
        "    cols_idxs = model.get_support(indices=True)\n",
        "    top_features = X.iloc[:,cols_idxs].columns\n",
        "\n",
        "    #Getting feature importance of the top features.\n",
        "    feature_importance_dict = {}\n",
        "    for feature in top_features:\n",
        "      id = X.columns.get_loc(feature)\n",
        "      feature_importance_dict[feature] = feature_importances[id]\n",
        "\n",
        "    feature_importance_dict = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    if len(feature_importance_dict) % 2 != 0:\n",
        "      feature_importance_dict.popitem()\n",
        "\n",
        "    x_labels = list(feature_importance_dict.keys())\n",
        "    y_values = list(feature_importance_dict.values())\n",
        "\n",
        "    # Plot the data using a bar plot\n",
        "    plt.figure(figsize=(8, 6))  # Set figure size\n",
        "    plt.bar(x_labels, y_values, color='skyblue')  # Create a bar plot\n",
        "\n",
        "    # Customize the plot\n",
        "    plt.title('Feature importance MDI')\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('Mean decrease impurity')\n",
        "\n",
        "    return x_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY7K8doyW_If"
      },
      "source": [
        "Finding top features to use for each group based on the remaining features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8yOC5TDW_If"
      },
      "outputs": [],
      "source": [
        "adl_list = groups['adl'].values\n",
        "\n",
        "for i in range(len(groups)):\n",
        "    data_copy = data[data['adl'].isin(adl_list[i])]\n",
        "    drop_features = [\"timestamp\", \"session\", \"subject\", \"fileID\"]\n",
        "    data_copy = data_copy.drop(columns=drop_features, axis=1)\n",
        "\n",
        "    important_features = get_important_features_GRF(data_copy)\n",
        "\n",
        "    print(\"Important features (\" + str(groups['windowSize'].values[i]) + '): ' + str(important_features))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
